import os
import sys

embeddings_path = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', 'embeddings'))

# Adiciona o caminho ao sys.path se ainda n√£o estiver l√°
if embeddings_path not in sys.path:
    sys.path.insert(0, embeddings_path) # Usar insert(0, ...) para dar prioridade

# -----------------------------------------------------------

# Agora, as importa√ß√µes padr√£o e as importa√ß√µes do seu m√≥dulo 'gerar_tudo'
import faiss
import numpy as np
import pandas as pd
import json
import logging
import streamlit as st
from sentence_transformers import SentenceTransformer

# A importa√ß√£o do 'gerar_tudo' agora deve funcionar
from gerar_tudo import extrair_texto_vaga, extrair_texto_candidato, extrair_texto_prospect

# Caminho base do projeto (onde est√° rodando este script)
BASE_DIR = os.path.dirname(os.path.abspath(__file__))

# Diret√≥rios de modelos e dados (um n√≠vel acima da pasta onde est√° este script)
MODEL_DIR = os.path.join(BASE_DIR, '..', 'models1')
DATA_DIR = os.path.join(BASE_DIR, '..', 'data')

# Nome do modelo de embeddings
EMBEDDING_MODEL_NAME = 'paraphrase-multilingual-mpnet-base-v2'

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# --- CARREGAMENTO DE RECURSOS GLOBAIS (CACHEADOS PELO STREAMLIT) ---
# O @st.cache_resource garante que estas fun√ß√µes sejam executadas APENAS UMA VEZ
# mesmo que o Streamlit re-execute o script (o que acontece frequentemente).

@st.cache_resource
def carregar_modelo_embedding():
    """Carrega o modelo de embedding uma √∫nica vez."""
    try:
        model = SentenceTransformer(EMBEDDING_MODEL_NAME)
        logging.info(f"Modelo de embedding '{EMBEDDING_MODEL_NAME}' carregado com sucesso.")
        return model
    except Exception as e:
        logging.error(f"Erro ao carregar o modelo de embedding: {e}. As buscas n√£o funcionar√£o.")
        st.warning(f"**Aviso:** O modelo de embedding n√£o p√¥de ser carregado. As funcionalidades de busca de similaridade estar√£o limitadas. Erro: {e}")
        return None

@st.cache_resource
def carregar_todos_dados_e_indices():
    """
    Carrega todos os dados originais, √≠ndices FAISS e metadados.
    Retorna um dicion√°rio com todos os recursos.
    """
    recursos = {
        "vagas_originais": {},
        "candidatos_originais": {},
        "prospects_data_list": [], # Carrega prospects como uma lista para facilitar a busca
        "index_vagas": None,
        "metadados_vagas": pd.DataFrame(),
        "index_candidatos": None,
        "metadados_candidatos": pd.DataFrame(),
        "index_prospects": None,
        "metadados_prospects": pd.DataFrame(),
    }

    # Fun√ß√µes auxiliares para carregar dados JSON e converter para dicion√°rio com ID
    def _carregar_json_para_dict(caminho_arquivo, id_key, default_prefix):
        try:
            with open(caminho_arquivo, 'r', encoding='utf-8') as f:
                data_raw = json.load(f)
                if isinstance(data_raw, list):
                    return {str(item.get(id_key, f"{default_prefix}_{i}")): item for i, item in enumerate(data_raw)}
                elif isinstance(data_raw, dict):
                    # Se for um √∫nico dicion√°rio no n√≠vel raiz, tentar usar o id_key
                    if id_key in data_raw:
                        return {str(data_raw[id_key]): data_raw}
                    else: # Caso n√£o tenha id_key, usar um ID an√¥nimo
                        return {f"{default_prefix}_0": data_raw}
                else:
                    logging.warning(f"Formato inesperado para {caminho_arquivo}. Esperado lista ou dicion√°rio.")
                    return {}
        except FileNotFoundError:
            logging.warning(f"Arquivo n√£o encontrado: {caminho_arquivo}")
            st.warning(f"**Aviso:** Arquivo de dados '{os.path.basename(caminho_arquivo)}' n√£o encontrado em '{DATA_DIR}'.")
            return {}
        except json.JSONDecodeError:
            logging.warning(f"Erro ao decodificar JSON: {caminho_arquivo}")
            st.error(f"**Erro:** Problema ao ler o arquivo JSON '{os.path.basename(caminho_arquivo)}'. Verifique o formato.")
            return {}
        except Exception as e:
            logging.warning(f"Erro inesperado ao carregar {caminho_arquivo}: {e}")
            st.error(f"**Erro inesperado** ao carregar '{os.path.basename(caminho_arquivo)}': {e}")
            return {}

    # Fun√ß√µes auxiliares para carregar FAISS e metadados
    def _carregar_faiss_index(nome_index):
        caminho_index = os.path.join(MODEL_DIR, f"index_{nome_index}.faiss")
        try:
            index = faiss.read_index(caminho_index)
            logging.info(f"√çndice '{nome_index}' carregado com sucesso.")
            return index
        except Exception as e:
            logging.error(f"Erro ao carregar o √≠ndice {caminho_index}: {e}")
            st.warning(f"**Aviso:** √çndice FAISS '{f'index_{nome_index}.faiss'}' n√£o encontrado ou corrompido em '{MODEL_DIR}'. As buscas de similaridade para {nome_index} podem n√£o funcionar.")
            return None

    def _carregar_metadados(nome_metadados):
        caminho_metadados = os.path.join(MODEL_DIR, f"{nome_metadados}_metadados.pkl")
        try:
            df = pd.read_pickle(caminho_metadados)
            logging.info(f"Metadados '{nome_metadados}' carregados com sucesso.")
            return df
        except Exception as e:
            logging.error(f"Erro ao carregar metadados {caminho_metadados}: {e}")
            st.warning(f"**Aviso:** Arquivo de metadados '{f'{nome_metadados}_metadados.pkl'}' n√£o encontrado ou corrompido em '{MODEL_DIR}'. As buscas de similaridade para {nome_metadados} podem n√£o funcionar corretamente.")
            return pd.DataFrame()

    # Carregamento dos dados originais
    recursos["vagas_originais"] = _carregar_json_para_dict(os.path.join(DATA_DIR, "vagas.json"), "id_vaga", "vaga_anon")
    recursos["candidatos_originais"] = _carregar_json_para_dict(os.path.join(DATA_DIR, "applicants.json"), "infos_basicas_codigo_profissional", "anon_cand")
    
    # Carrega prospects como uma lista para facilitar a busca por candidato/vaga
    try:
        with open(os.path.join(DATA_DIR, "prospects.json"), 'r', encoding='utf-8') as f:
            recursos["prospects_data_list"] = json.load(f) # Carrega como lista
        logging.info("Dados de prospects carregados como lista.")
    except FileNotFoundError:
        logging.warning(f"Arquivo n√£o encontrado: {os.path.join(DATA_DIR, 'prospects.json')}")
        st.warning(f"**Aviso:** Arquivo de prospects 'prospects.json' n√£o encontrado em '{DATA_DIR}'. A pontua√ß√£o de hist√≥rico estar√° indispon√≠vel.")
        recursos["prospects_data_list"] = []
    except json.JSONDecodeError:
        logging.warning(f"Erro ao decodificar JSON: {os.path.join(DATA_DIR, 'prospects.json')}")
        st.error(f"**Erro:** Problema ao ler o arquivo JSON 'prospects.json'. Verifique o formato.")
        recursos["prospects_data_list"] = []
    except Exception as e:
        logging.error(f"Erro ao carregar prospects.json como lista: {e}")
        st.error(f"**Erro inesperado** ao carregar 'prospects.json': {e}")
        recursos["prospects_data_list"] = []

    # Carregamento dos √≠ndices e metadados
    # Estes ser√£o None ou DataFrame vazios se os arquivos n√£o existirem/estiverem corrompidos
    recursos["index_vagas"] = _carregar_faiss_index("vagas")
    recursos["metadados_vagas"] = _carregar_metadados("vagas")

    recursos["index_candidatos"] = _carregar_faiss_index("candidatos")
    recursos["metadados_candidatos"] = _carregar_metadados("candidatos")

    recursos["index_prospects"] = _carregar_faiss_index("prospects")
    recursos["metadados_prospects"] = _carregar_metadados("prospects")

    return recursos

# Carrega os recursos (modelo, dados, √≠ndices) uma √∫nica vez ao iniciar a aplica√ß√£o
embedding_model = carregar_modelo_embedding()
recursos_carregados = carregar_todos_dados_e_indices()

# Atribui os recursos para uso f√°cil
vagas_originais = recursos_carregados["vagas_originais"]
candidatos_originais = recursos_carregados["candidatos_originais"]
prospects_data_list = recursos_carregados["prospects_data_list"] # Agora √© uma lista
index_vagas = recursos_carregados["index_vagas"]
metadados_vagas = recursos_carregados["metadados_vagas"]
index_candidatos = recursos_carregados["index_candidatos"]
metadados_candidatos = recursos_carregados["metadados_candidatos"] 
index_prospects = recursos_carregados["index_prospects"]
metadados_prospects = recursos_carregados["metadados_prospects"]


# --- FUN√á√ïES DE BUSCA DE SIMILARIDADE ---

def buscar_similares(query_embedding, faiss_index, metadados_df, k=10):
    """
    Realiza a busca de similaridade no √≠ndice FAISS.
    Args:
        query_embedding (np.array): O embedding da query (vaga ou candidato).
        faiss_index (faiss.Index): O √≠ndice FAISS onde a busca ser√° feita.
        metadados_df (pd.DataFrame): DataFrame com os metadados correspondentes ao √≠ndice.
        k (int): N√∫mero de resultados a retornar.
    Returns:
        list: Uma lista de dicion√°rios contendo os resultados da busca (id_original, dist√¢ncia).
    """
    if faiss_index is None or metadados_df.empty or query_embedding is None:
        logging.warning("√çndice, metadados ou embedding da query inv√°lido para busca. Retornando lista vazia.")
        st.warning("**Aviso:** Funcionalidade de busca de similaridade n√£o dispon√≠vel. Verifique se os arquivos de √≠ndice e metadados foram carregados corretamente.")
        return []

    # Assegura que query_embedding √© um array 2D para faiss.search
    query_embedding = np.array([query_embedding]).astype(np.float32)

    try:
        distances, indices = faiss_index.search(query_embedding, k)
        resultados = []
        for dist, idx_faiss in zip(distances[0], indices[0]):
            if idx_faiss != -1 and idx_faiss < len(metadados_df): # Valida se o √≠ndice retornado √© v√°lido e dentro dos limites do DataFrame
                original_id = metadados_df.iloc[idx_faiss]['id_original']
                resultados.append({"id_original": original_id, "distancia": float(dist)})
        return resultados
    except Exception as e:
        logging.error(f"Erro durante a busca FAISS: {e}")
        st.error(f"**Erro:** N√£o foi poss√≠vel realizar a busca de similaridade. Detalhes: {e}")
        return []

def calcular_pontuacao_historico(candidato_id, prospects_data):
    """
    Calcula uma pontua√ß√£o de hist√≥rico para um candidato com base nas suas situa√ß√µes em vagas.
    Maior pontua√ß√£o para situa√ß√µes mais positivas.
    """
    if not prospects_data: # Verifica se a lista de prospects est√° vazia
        return 0 # Nenhuma entrada de hist√≥rico para este candidato

    pontuacoes = {
        "Contratado": 10,
        "Encaminhado ao Requisitante": 8,
        "Entrevista com Cliente": 7,
        "Em Negocia√ß√£o": 6,
        "Em Andamento": 3,
        "Aguardando Contato": 2,
        "Em avalia√ß√£o pelo RH": -1, # Ligeiramente negativo para indicar que est√° em outro processo
        "Desistiu": -5,
        "Rejeitado": -8,
        "N√£o Atende aos Requisitos": -10,
        "Outros": 0 # Situa√ß√µes n√£o mapeadas
    }
    
    historico_pontos = []
    
    # Filtrar os prospects relevantes para o candidato
    prospects_do_candidato = [p for p in prospects_data if str(p.get("prospect_codigo")) == str(candidato_id)]
    
    if not prospects_do_candidato:
        return 0 # Nenhuma entrada de hist√≥rico para este candidato

    for prospect in prospects_do_candidato:
        situacao = prospect.get("prospect_situacao_candidado", "Outros")
        historico_pontos.append(pontuacoes.get(situacao, pontuacoes["Outros"]))
    
    # Retorna a m√©dia das pontua√ß√µes do hist√≥rico do candidato
    return np.mean(historico_pontos) if historico_pontos else 0

def encontrar_candidatos_para_vaga(id_vaga, num_candidatos=5, peso_historico=0.3): # Valor padr√£o de 0.3 (30%)
    """
    Busca candidatos aderentes a uma vaga espec√≠fica, calculando a pontua√ß√£o de ader√™ncia
    e ponderando pelo hist√≥rico do candidato.
    """
    if embedding_model is None:
        return {"erro": "Modelo de embedding n√£o carregado. N√£o √© poss√≠vel realizar a busca de similaridade."}
    
    if index_candidatos is None or metadados_candidatos.empty:
        return {"erro": "√çndice de candidatos ou metadados n√£o carregados. N√£o √© poss√≠vel realizar a busca de similaridade."}

    vaga_data = vagas_originais.get(str(id_vaga))
    if not vaga_data:
        logging.warning(f"Vaga com ID '{id_vaga}' n√£o encontrada nos dados originais.")
        return {"erro": f"Vaga com ID '{id_vaga}' n√£o encontrada."}

    texto_vaga = extrair_texto_vaga(vaga_data)
    if not texto_vaga:
        logging.warning(f"Vaga ID '{id_vaga}' n√£o possui texto √∫til para gerar query embedding.")
        return {"erro": "Informa√ß√µes insuficientes na vaga para realizar a busca."}

    try:
        query_embedding = embedding_model.encode([texto_vaga])[0].astype(np.float32)
    except Exception as e:
        logging.error(f"Erro ao gerar embedding para a vaga: {e}")
        return {"erro": f"Erro ao gerar embedding para a vaga. Detalhes: {e}"}


    # Buscar mais candidatos do que o necess√°rio inicialmente para filtrar e ordenar
    resultados_similares = buscar_similares(query_embedding, index_candidatos, metadados_candidatos, k=num_candidatos * 5) # Buscamos mais para ter hist√≥rico

    if not resultados_similares:
        return [] # Retorna lista vazia se nenhuma similaridade for encontrada

    candidatos_encontrados = []
    
    # Encontrar a dist√¢ncia m√°xima para normaliza√ß√£o (se houver resultados)
    # Consideramos apenas dist√¢ncias positivas para evitar problemas em caso de 0
    distancias_validas = [res['distancia'] for res in resultados_similares if res['distancia'] >= 0] # Distancia pode ser 0 em match perfeito
    max_distancia = max(distancias_validas) if distancias_validas else 1.0 # Garante que n√£o √© zero

    for res in resultados_similares:
        candidato_id = str(res['id_original'])
        candidato_detalhes = candidatos_originais.get(candidato_id)
        if candidato_detalhes:
            # Calcular Pontua√ß√£o de Ader√™ncia (baseada em similaridade textual)
            if max_distancia > 0:
                # Quanto menor a dist√¢ncia, maior a similaridade. Invertemos para pontua√ß√£o: (1 - dist/max_dist)
                pontuacao_aderencia_similaridade = (1 - (res['distancia'] / max_distancia)) * 100
            else: # Se todas as dist√¢ncias forem zero (match perfeito ou apenas um resultado com dist=0)
                pontuacao_aderencia_similaridade = 100 if res['distancia'] == 0 else 0 

            # Calcular Pontua√ß√£o de Hist√≥rico
            pontuacao_historico = calcular_pontuacao_historico(candidato_id, prospects_data_list)
            
            # Normalizar pontua√ß√£o de hist√≥rico para uma escala de 0-100
            # Nossas pontua√ß√µes v√£o de -10 a 10. Reescalamos para (x - min) / (max - min) * 100
            min_hist_score = -10 
            max_hist_score = 10  
            
            if (max_hist_score - min_hist_score) > 0:
                pontuacao_historico_normalizada = ((pontuacao_historico - min_hist_score) / (max_hist_score - min_hist_score)) * 100
            else:
                pontuacao_historico_normalizada = 50 # Neutro se n√£o houver varia√ß√£o na pontua√ß√£o

            # Pontua√ß√£o Final Ponderada
            pontuacao_final = (pontuacao_aderencia_similaridade * (1 - peso_historico)) + \
                               (pontuacao_historico_normalizada * peso_historico)
            
            # Garante que a pontua√ß√£o final esteja entre 0 e 100
            pontuacao_final = max(0, min(100, pontuacao_final))

            candidatos_encontrados.append({
                "id_candidato": candidato_id,
                "Pontua√ß√£o Final de Ader√™ncia (0-100)": round(pontuacao_final, 2), 
                # As pontua√ß√µes abaixo ser√£o usadas apenas internamente para depura√ß√£o ou an√°lises futuras,
                # n√£o ser√£o exibidas na interface para simplificar.
                "Pontua√ß√£o de Similaridade (0-100)_debug": round(pontuacao_aderencia_similaridade, 2),
                "Pontua√ß√£o de Hist√≥rico (M√©dia)_debug": round(pontuacao_historico, 2), 
                "Nome do Profissional": candidato_detalhes.get("infos_basicas_nome", "Nome n√£o dispon√≠vel"),
                "Email": candidato_detalhes.get("infos_basicas_email", "N√£o informado"),
                "Telefone": candidato_detalhes.get("infos_basicas_telefone", "N√£o informado"),
                "T√≠tulo Profissional": candidato_detalhes.get("informacoes_profissionais_titulo_profissional", "N√£o informado"),
                "Dist√¢ncia Euclidiana (Refer√™ncia)": res['distancia'], 
                "Dados Completos": json.dumps(candidato_detalhes, ensure_ascii=False, indent=2) 
            })
    
    # Ordenar pela Pontua√ß√£o Final de Ader√™ncia (decrescente = mais aderente) e pegar os top N
    candidatos_encontrados_df = pd.DataFrame(candidatos_encontrados)
    if not candidatos_encontrados_df.empty:
        candidatos_encontrados_df = candidatos_encontrados_df.sort_values(by="Pontua√ß√£o Final de Ader√™ncia (0-100)", ascending=False).head(num_candidatos)
    
    return candidatos_encontrados_df.to_dict(orient='records')


# --- INTERFACE STREAMLIT ---

def pagina_servicos():
    st.title("Sistema de Recomenda√ß√£o de Talentos")
    st.markdown("Bem-vindo(a)! Utilize o sistema para encontrar os **candidatos mais aderentes a uma vaga**, considerando sua similaridade com a descri√ß√£o da vaga e seu hist√≥rico de engajamento em processos seletivos anteriores.")

    # Exibe m√©tricas de quantos itens foram carregados
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric(label="Total de Vagas", value=len(vagas_originais))
    with col2:
        st.metric(label="Total de Candidatos", value=len(candidatos_originais))
    with col3:
        # Contamos os prospects √∫nicos por prospect_codigo para exibir um n√∫mero mais realista de "candidatos prospectados"
        unique_prospects = len(set([p.get("prospect_codigo") for p in prospects_data_list if p.get("prospect_codigo")]))
        st.metric(label="Total de Intera√ß√µes de Prospects", value=len(prospects_data_list), help="N√∫mero de registros de intera√ß√£o (candidato-vaga) no hist√≥rico de prospects.")
        # Opcional: st.metric(label="Prospects √önicos", value=unique_prospects)

    st.markdown("---")

    # --- Se√ß√£o principal: Encontrar Candidatos para uma Vaga ---
    st.header("üéØ Encontrar Candidatos para uma Vaga")
    st.markdown("Para encontrar os candidatos ideais, insira o **ID da vaga** e o **n√∫mero de candidatos** que deseja exibir.")

    with st.form("form_vaga_candidato"):
        col_vaga1, col_vaga2 = st.columns([0.7, 0.3]) # Colunas ajustadas
        with col_vaga1:
            vaga_id_input = st.text_input("ID da Vaga", help="Ex: 1055, 11589").strip()
        with col_vaga2:
            num_candidatos_input = st.slider("N√∫mero de Candidatos a exibir", min_value=1, max_value=20, value=5)
        
        # O peso do hist√≥rico √© fixado no backend, n√£o mais na interface
        peso_historico_normalized = 0.3 # <--- PESO DO HIST√ìRICO PADR√ÉO DEFINIDO AQUI (30%)

        submit_vaga_candidato = st.form_submit_button("Buscar Candidatos")

    if submit_vaga_candidato:
        if vaga_id_input:
            with st.spinner("Buscando candidatos e analisando ader√™ncia..."):
                vaga_info = vagas_originais.get(vaga_id_input)
                if not vaga_info:
                    st.error(f"Vaga com ID '{vaga_id_input}' n√£o encontrada. Por favor, verifique o ID.")
                else:
                    st.subheader(f"Resultados encontrados para a Vaga ID: {vaga_id_input}")
                    st.markdown(f"**Vaga:** {vaga_info.get('info_titulo_vaga', 'N√£o dispon√≠vel')}")
                    st.markdown(f"**Cliente:** {vaga_info.get('info_cliente', 'N√£o informado')}")
                    
                    # --- TRECHO ATUALIZADO: Extra√ß√£o e exibi√ß√£o da descri√ß√£o COMPLETA da vaga ---
                    vaga_description_parts = []
                    # Priorizamos 'perfil_principais_atividades' por ser o mais comum para descri√ß√£o
                    if vaga_info.get('perfil_principais_atividades'):
                        vaga_description_parts.append(vaga_info['perfil_principais_atividades'])
                    
                    # Adicionamos outras partes relevantes se existirem, para garantir a descri√ß√£o completa
                    if vaga_info.get('perfil_competencia_tecnicas_e_comportamentais'):
                        vaga_description_parts.append("\n\n**Compet√™ncias T√©cnicas e Comportamentais:**\n" + vaga_info['perfil_competencia_tecnicas_e_comportamentais'])
                    if vaga_info.get('perfil_demais_observacoes'):
                        vaga_description_parts.append("\n\n**Outras Observa√ß√µes:**\n" + vaga_info['perfil_demais_observacoes'])
                    
                    full_description = "\n".join(filter(None, (p.strip() for p in vaga_description_parts))).strip()

                    if full_description:
                        st.markdown(f"**Descri√ß√£o:**\n{full_description}") # Exibe a descri√ß√£o completa
                    else:
                        st.markdown("**Descri√ß√£o:** N√£o dispon√≠vel.")
                    # --- FIM TRECHO ATUALIZADO ---

                    resultados_df_raw = encontrar_candidatos_para_vaga(vaga_id_input, num_candidatos_input, peso_historico_normalized)
                    
                    if isinstance(resultados_df_raw, dict) and "erro" in resultados_df_raw:
                        st.error(resultados_df_raw["erro"])
                    elif not resultados_df_raw:
                        st.info("Nenhum candidato similar encontrado para esta vaga com os crit√©rios atuais ou funcionalidades de busca n√£o dispon√≠veis.")
                    else:
                        # Converter para DataFrame para melhor visualiza√ß√£o e manipula√ß√£o
                        resultados_df = pd.DataFrame(resultados_df_raw)
                        
                        # Reordenar colunas para a visualiza√ß√£o, exibindo apenas a pontua√ß√£o final
                        cols_display = [
                            "Nome do Profissional", 
                            "Email", 
                            "Telefone", 
                            "Pontua√ß√£o Final de Ader√™ncia (0-100)", 
                            "id_candidato"
                        ]
                        resultados_df_display = resultados_df[cols_display].copy()
                        
                        # Formatar coluna de pontua√ß√£o para 2 casas decimais
                        resultados_df_display["Pontua√ß√£o Final de Ader√™ncia (0-100)"] = resultados_df_display["Pontua√ß√£o Final de Ader√™ncia (0-100)"].apply(lambda x: f"{x:.2f}")

                        st.markdown("---")
                        st.write("Abaixo est√£o os candidatos mais aderentes √† vaga, ordenados pela **maior Pontua√ß√£o Final de Ader√™ncia**:")
                        st.dataframe(resultados_df_display.set_index('id_candidato'), use_container_width=True)

                        # Bot√£o de download para todos os resultados
                        # NOTA: O CSV de download ainda ter√° as colunas de debug para an√°lise se necess√°rio,
                        # mas n√£o ser√£o vis√≠veis na tela.
                        csv_data = resultados_df.drop(columns=["Dados Completos", "Pontua√ß√£o de Similaridade (0-100)_debug", "Pontua√ß√£o de Hist√≥rico (M√©dia)_debug"]).to_csv(index=False).encode('utf-8') 
                        st.download_button(
                            label="Download de Todos os Dados dos Candidatos (CSV)",
                            data=csv_data,
                            file_name=f"candidatos_vaga_{vaga_id_input}.csv",
                            mime="text/csv",
                            key=f"download_csv_vaga_{vaga_id_input}"
                        )

                        # Adicionar op√ß√£o de download individual (JSON) para cada candidato
                        st.markdown("---")
                        st.subheader("Download individual dos dados completos dos candidatos:")
                        for idx, row in resultados_df.iterrows():
                            col_name, col_download = st.columns([0.7, 0.3])
                            with col_name:
                                st.write(f"**{row['Nome do Profissional']}** (ID: {row['id_candidato']})")
                            with col_download:
                                st.download_button(
                                    label="Download JSON",
                                    data=row['Dados Completos'].encode('utf-8'),
                                    file_name=f"candidato_{row['id_candidato']}.json",
                                    mime="application/json",
                                    key=f"download_json_cand_{row['id_candidato']}"
                                )
        else:
            st.warning("Por favor, insira um ID de vaga para buscar.")

    st.markdown("---")
